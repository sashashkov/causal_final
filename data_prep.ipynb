{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb5bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23511338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add partisan lean and get US state info from location\n",
    "\n",
    "# dataframe containing state name, state abbrevation, and partisan lean for each US state\n",
    "lean_df = pd.read_csv('partisan_lean.csv')\n",
    "\n",
    "\n",
    "def get_partisan_lean(user_location):\n",
    "    count = 0\n",
    "    user_lean = -1\n",
    "    state = ''\n",
    "    \n",
    "    # loop through states\n",
    "    for index, row in lean_df.iterrows():\n",
    "        # there's a couple nans in the dataset\n",
    "        if type(user_location) != type('a'):\n",
    "            continue\n",
    "        # check if abbreviation is in string\n",
    "        if ' ' + row['abbrev'] in user_location:\n",
    "            user_lean = row['vote_share']\n",
    "            count += 1\n",
    "            state = row['state'].title()\n",
    "        \n",
    "        # check if state name is in string\n",
    "        if row['state'] in user_location.upper():\n",
    "            # very rarely a city will have the same name as a state, this prioritizes using the abbreviation\n",
    "            if count < 1:\n",
    "                user_lean = row['vote_share']\n",
    "            count+=1\n",
    "            state = row['state'].title()\n",
    "\n",
    "    \n",
    "    return user_lean, state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c0770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file containing demographic data for each state\n",
    "demo_data = pd.read_csv('state_demographics.csv')\n",
    "\n",
    "# the specific variables we want to include\n",
    "attributes = ['Age.Percent Under 5 Years', 'Age.Percent Under 18 Years',\n",
    "           'Age.Percent 65 and Older', 'Miscellaneous.Percent Female',\n",
    "           'Ethnicities.White Alone', 'Ethnicities.Black Alone', \n",
    "            'Ethnicities.Hispanic or Latino', 'Education.Bachelor\\'s Degree or Higher']\n",
    "\n",
    "# get demographic data from state name\n",
    "def get_demographic_data(state):\n",
    "    state_data = demo_data[demo_data['State'] == state].iloc[0][attributes]\n",
    "    return state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91427962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# reformat the variable with a string containing the tweet date into a datetime object\n",
    "def get_date_from_tweet(s):\n",
    "    parts = s.split()\n",
    "    datestring = parts[1] + ' ' + parts[2] + ' ' + parts[-1]\n",
    "    return datetime.strptime(datestring, '%b %d %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559b0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file containing daily vaccine data from US states\n",
    "vaccine_data = pd.read_csv('us_state_vaccinations.csv')\n",
    "\n",
    "# variable we want to keep from day of tweet\n",
    "vax_vars = ['people_vaccinated_per_hundred', 'daily_vaccinations_per_million']\n",
    "# variable for daily vax rate\n",
    "vax_per_day = 'daily_vaccinations_per_million'\n",
    "\n",
    "def get_vaccine_data(state, time):\n",
    "    # because my life was too easy\n",
    "    if state == 'New York':\n",
    "        state = 'New York State'\n",
    "        \n",
    "    state_data = vaccine_data[vaccine_data['location'] == state]\n",
    "    \n",
    "    # data for date of tweet\n",
    "    today = state_data[state_data['date'] == time.strftime('%Y-%m-%d')].iloc[0][vax_vars]\n",
    "        \n",
    "    \n",
    "    # one week before\n",
    "    week_prior_time = time - timedelta(days=7)\n",
    "    week_prior = state_data[state_data['date'] == week_prior_time.strftime('%Y-%m-%d')].iloc[0][vax_per_day]\n",
    "    \n",
    "    # one week after\n",
    "    week_future_time = time + timedelta(days=7)\n",
    "    week_future = state_data[state_data['date'] == week_future_time.strftime('%Y-%m-%d')].iloc[0][vax_per_day]\n",
    "    \n",
    "    # two weeks after\n",
    "    two_week_future_time = time + timedelta(days=14)\n",
    "    two_week_future = state_data[state_data['date'] == two_week_future_time.strftime('%Y-%m-%d')].iloc[0][vax_per_day]\n",
    "    \n",
    "    # reformat and return\n",
    "    collected = *today , week_prior ,week_future,  two_week_future\n",
    "    return pd.Series(collected)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b294873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv containing daily vaccination data for a bunch of different countries\n",
    "c_data = pd.read_csv('country_vaccinations.csv')\n",
    "# only keep US data\n",
    "c_data = c_data[c_data['iso_code'] == 'USA']\n",
    "\n",
    "# variable we want to keep from day of tweet\n",
    "c_vax_vars = ['people_vaccinated_per_hundred', 'daily_vaccinations_per_million']\n",
    "# variable for daily vax rate\n",
    "c_vax_per_day = 'daily_vaccinations_per_million'\n",
    "\n",
    "def get_national_data(time):\n",
    "    \n",
    "    # data for date of tweet\n",
    "    today = c_data[c_data['date'] == time.strftime('%Y-%m-%d')].iloc[0][c_vax_vars]\n",
    "        \n",
    "    \n",
    "    # one week before\n",
    "    week_prior_time = time - timedelta(days=7)\n",
    "    week_prior = c_data[c_data['date'] == week_prior_time.strftime('%Y-%m-%d')].iloc[0][c_vax_per_day]\n",
    "    \n",
    "    # one week after\n",
    "    week_future_time = time + timedelta(days=7)\n",
    "    week_future = c_data[c_data['date'] == week_future_time.strftime('%Y-%m-%d')].iloc[0][c_vax_per_day]\n",
    "    \n",
    "    # two weeks after\n",
    "    two_week_future_time = time + timedelta(days=14)\n",
    "    two_week_future = c_data[c_data['date'] == two_week_future_time.strftime('%Y-%m-%d')].iloc[0][c_vax_per_day]\n",
    "    \n",
    "    # reformat and return\n",
    "    collected = *today, week_prior, week_future, two_week_future\n",
    "    return pd.Series(collected)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6010da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date when vaccine lottery was announced in each state\n",
    "lottery_start_dates = {'Arkansas' : datetime(2021, 5, 25),\n",
    "                       'California' : datetime(2021, 5, 27),\n",
    "                       'Kentucky' : datetime(2021, 6, 4),\n",
    "                       'Maryland' : datetime(2021, 5, 20),\n",
    "                       'New Mexico' : datetime(2021, 6, 1),\n",
    "                       'New York' : datetime(2021, 5, 20),\n",
    "                       'Ohio' : datetime(2021, 5, 12),\n",
    "                       'Oregon' : datetime(2021, 5, 21),\n",
    "                       'Washington': datetime(2021, 6, 3),\n",
    "                       'West Virginia': datetime(2021, 5, 27)\n",
    "                      }\n",
    "\n",
    "\n",
    "# check if tweet occured within 30 days of announcement of vaccine lottery\n",
    "def during_lottery(state, time):\n",
    "    if state not in lottery_start_dates.keys():\n",
    "        return 0\n",
    "    \n",
    "    return (time - lottery_start_dates[state] > timedelta(days = 0)) and (time - lottery_start_dates[state] < timedelta(days = 30))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1442e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be dropped at the end of processing\n",
    "# the only used columns are created_at, id, place\n",
    "\n",
    "dropped = ['coordinates', 'created_at', 'hashtags', 'media', 'urls',\n",
    "       'favorite_count', 'id', 'in_reply_to_screen_name',\n",
    "       'in_reply_to_status_id', 'in_reply_to_user_id', 'lang', 'place',\n",
    "       'possibly_sensitive', 'quote_id', 'retweet_count', 'retweet_id',\n",
    "       'retweet_screen_name', 'source', 'text', 'tweet_url', 'user_created_at',\n",
    "       'user_id', 'user_default_profile_image', 'user_description',\n",
    "       'user_favourites_count', 'user_followers_count', 'user_friends_count',\n",
    "       'user_listed_count', 'user_location', 'user_name', 'user_screen_name',\n",
    "       'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified']\n",
    "\n",
    "def process_csv(filename):\n",
    "    df = pd.read_csv(os.path.join('data_hydrated', filename))\n",
    "    if 'may' in filename:\n",
    "        directory = 'data/2021_05'\n",
    "    else:\n",
    "        directory = 'data/2021_06'\n",
    "    sentiments = pd.read_csv(os.path.join(directory, filename), header = None)\n",
    "    sentiments.columns = ['id', 'sentiment']\n",
    "\n",
    "    # add sentiments back into hydrated DataFrame\n",
    "    df_with_sentiments = pd.merge(df, sentiments, on=['id'], how='inner')\n",
    "    \n",
    "    # add partisan lean and state name to DataFrame\n",
    "    df_with_sentiments[['partisan_lean', 'state']] = df_with_sentiments['place'].apply(lambda x: pd.Series(get_partisan_lean(x)))\n",
    "    \n",
    "    # remove non US and non-identifiable entries\n",
    "    df_us = df_with_sentiments[df_with_sentiments['partisan_lean'] != -1]\n",
    "    \n",
    "    # add demographic data to tweets\n",
    "    df_us[attributes] = df_us['state'].apply(get_demographic_data)\n",
    "    \n",
    "    # add python datetime to tweets\n",
    "    df_us['date'] = df_us['created_at'].apply(get_date_from_tweet)\n",
    "    \n",
    "    # add state level vaccine data\n",
    "    df_us[['state percent vaccinated', 'state_rate_0', 'state_rate-7', 'state_rate+7', 'state_rate+14']] = df_us.apply(lambda row: get_vaccine_data(row['state'], row['date']), axis=1)\n",
    "    \n",
    "    # add national level vaccine data\n",
    "    df_us[['US percent vaccinated', 'US_rate_0', 'US_rate-7', 'US_rate+7', 'US_rate+14']] = df_us['date'].apply(get_national_data)\n",
    "    \n",
    "    df_us['lottery'] = df_us.apply(lambda row: during_lottery(row['state'], row['date']), axis = 1)\n",
    "    \n",
    "    df_us = df_us.drop(columns = dropped)\n",
    "    return df_us\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80521cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_june24_june25.csv\n",
      "2021_may23_may24.csv\n",
      "2021_may1_may2.csv\n",
      "2021_june29_june30.csv\n",
      "2021_june21_june22.csv\n",
      "2021_may13_may14.csv\n",
      "2021_june27_june28.csv\n",
      "2021_june5_june6.csv\n",
      "2021_june9_june10.csv\n",
      "2021_may15_may16.csv\n",
      "2021_june10_june11.csv\n",
      "2021_june3_june4.csv\n",
      "2021_june15_june16.csv\n",
      "2021_may25_may26.csv\n",
      "2021_june2_june3.csv\n",
      "2021_may17_may18.csv\n",
      "2021_june28_june29.csv\n",
      "2021_june4_june5.csv\n",
      "2021_june19_june20.csv\n",
      "2021_may27_may28.csv\n",
      "2021_may6_may7.csv\n",
      "2021_june26_june27.csv\n",
      "2021_may22_may23.csv\n",
      "2021_may24_may25.csv\n",
      "2021_may30_may31.csv\n",
      "2021_may31_june1.csv\n",
      "2021_may29_may30.csv\n",
      "2021_may14_may15.csv\n",
      "2021_may8_may9.csv\n",
      "2021_june23_june24.csv\n",
      "2021_june7_june8.csv\n",
      "2021_may12_may13.csv\n",
      "2021_june12_june13.csv\n",
      "2021_june14_june15.csv\n",
      "2021_june1_june2.csv\n",
      "2021_june17_june18.csv\n",
      "2021_may3_may4.csv\n",
      "2021_june11_june12.csv\n",
      "2021_may21_may22.csv\n",
      "2021_may19_may20.csv\n",
      "2021_june20_june21.csv\n",
      "2021_may2_may3.csv\n",
      "2021_may7_may8.csv\n",
      "2021_june25_june26.csv\n",
      "2021_may11_may12.csv\n",
      "2021_june30_july1.csv\n",
      "2021_june18_june19.csv\n",
      "2021_may9_may10.csv\n",
      "2021_may28_may29.csv\n",
      "2021_june16_june17.csv\n",
      "2021_may18_may19.csv\n",
      "2021_june6_june7.csv\n",
      "2021_may16_may17.csv\n",
      "2021_may5_may6.csv\n",
      "2021_may10_may11.csv\n",
      "2021_june8_june9.csv\n",
      "2021_june22_june23.csv\n",
      "2021_may4_may5.csv\n",
      "2021_may20_may21.csv\n",
      "2021_june13_june14.csv\n",
      "2021_may26_may27.csv\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "\n",
    "for filename in os.listdir('data_hydrated'):\n",
    "    print(filename)\n",
    "    df_list.append(process_csv(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5455c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.to_csv('full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b7fd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1420, 23), (6193, 23))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lottery'] == 1].shape, df[df['lottery'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dc7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
